{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:39.345733",
     "start_time": "2016-03-15T22:07:34.892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType, StringType, ArrayType, StructType, StructField}\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer, VectorAssembler, Normalizer}\n",
    "import org.apache.spark.ml.evaluation.{RegressionEvaluator, MulticlassClassificationEvaluator, BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier, DecisionTreeClassifier, GBTClassifier}\n",
    "\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "\n",
    "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n",
    "\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "\n",
    "import collection.mutable._\n",
    "import java.io._\n",
    "import scala.math.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:39.366483",
     "start_time": "2016-03-15T22:07:34.901Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:41.080921",
     "start_time": "2016-03-15T22:07:34.915Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Log Loss metric\n",
    "def logloss(df:DataFrame) : Double = {\n",
    "    var loglossRed1 = df.map(r => (r.getAs[Double](\"indexedLabel\"), r.getAs[Double](\"proba\"))) //probability[1]\n",
    "    \n",
    "    // Check if(some proba are < 0\n",
    "    val neg = loglossRed1.filter{ case (y,p) => (p <= 0.0 || p >= 1.0)}\n",
    "    val negCount = neg.count()\n",
    "    if(negCount != 0) {\n",
    "        println(\"!!! There so non-valid probability !!! \" + negCount)\n",
    "        loglossRed1 = loglossRed1.filter{ case (y,p) => (p > 0.0 && p < 1.0)}\n",
    "    }\n",
    "    \n",
    "    val loglossRed2 =  loglossRed1.map{case (y,p) => y*log(p) + (1-y)*log(1.0-p)}\n",
    "    val loglossRed  =  loglossRed2.reduce((a, b) => a+b)\n",
    "    \n",
    "    return -1.0 * loglossRed / df.count()\n",
    "}\n",
    "//println(\"Logloss on Training \" + logloss(trainPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:42.426684",
     "start_time": "2016-03-15T22:07:34.923Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Shrink dow extrem proba\n",
    "def shrink(_value:Double, _factor:Option[Double] = Some(0.2), trunc:Double = 0.1) : Double = {\n",
    "    var value = _value\n",
    "    \n",
    "    if(value < 0.0 + trunc) {\n",
    "        value = 0.0 + trunc\n",
    "    }\n",
    "    else if(value > 1.0 - trunc) {\n",
    "        value = 1.0 - trunc\n",
    "    }\n",
    "        \n",
    "    if(_factor == None) {\n",
    "        return value\n",
    "    }\n",
    "    var factor:Double = _factor.get\n",
    "    \n",
    "    return value * (1.0-factor) + factor/2.0\n",
    "}\n",
    "//println(shrink(0.5))\n",
    "//println(shrink(0))\n",
    "//println(shrink(1))\n",
    "//println(shrink(0.95))\n",
    "//println(shrink(0.05))\n",
    "\n",
    "def shrinkDf(df:DataFrame, factor:Option[Double] = Some(0.2), trunc:Double = 0.0) : DataFrame = {\n",
    "    // proba=u\"[1,null,null,[0.9413866396761132,0.05861336032388664]]\n",
    "    //val shrinkUdf = udf(probability => shrink(float(probability.split(\",\")(4)(-2), factor, trunc), DoubleType()))\n",
    "    \n",
    "    val coder: (Vector => Double) = (probability: Vector) => {shrink(probability(1), factor, trunc)}\n",
    "    val shrinkUdf = udf(coder)\n",
    "    \n",
    "    //val dfShrink1 = df.withColumn(\"proba\", df(\"probability\"))\n",
    "    //println(dfShrink1.take(1)\n",
    "    //val dfShrink = dfShrink1.withColumn(\"proba\", shrinkUdf(dfShrink1(\"proba\")))\n",
    "    val dfShrink = df.withColumn(\"proba\", shrinkUdf(df(\"probability\")))\n",
    "    \n",
    "    return dfShrink\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:42.505077",
     "start_time": "2016-03-15T22:07:34.933Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:46: error: not found: value trainPredictions\n",
       "              trainPredictions.select(\"probability\").take(5).foreach(println)\n",
       "              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPredictions.select(\"probability\").take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:42.609825",
     "start_time": "2016-03-15T22:07:34.942Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:49: error: not found: value trainPredictions\n",
       "         val trainPredictionsShrink = shrinkDf(trainPredictions, Some(0.3))\n",
       "                                               ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainPredictionsShrink = shrinkDf(trainPredictions, Some(0.3))\n",
    "trainPredictionsShrink.select(\"probability\", \"proba\").take(5) //.select(\"proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:49.536802",
     "start_time": "2016-03-15T22:07:34.952Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, target: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, v59: d..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kaggleTrain = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"kaggle/train.csv\")\n",
    "\n",
    "kaggleTrain.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:07:53.256774",
     "start_time": "2016-03-15T22:07:34.959Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val kaggleTest = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"kaggle/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:08:06.103881",
     "start_time": "2016-03-15T22:07:34.967Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Train count 114321\n",
      "Kaggle Test count  114393\n"
     ]
    }
   ],
   "source": [
    "println(\"Kaggle Train count \" + kaggleTrain.count())\n",
    "println(\"Kaggle Test count  \" + kaggleTest.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:08:07.315268",
     "start_time": "2016-03-15T22:07:34.974Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the ID columns, type IntegerType\n",
      "We have the target columns, type IntegerType\n",
      "DoubleType 108\n",
      "StringType 19\n",
      "IntegerType 4\n"
     ]
    }
   ],
   "source": [
    "//println(train.schema.fields\n",
    "var columnsDict = new HashMap[String,List[String]].withDefaultValue(Nil)\n",
    "for( col <- kaggleTrain.schema.fields ) {\n",
    "    val typeKey = col.dataType\n",
    "    val colName = col.name\n",
    "    \n",
    "    if(colName == \"ID\") {\n",
    "        println(\"We have the ID columns, type \" + typeKey)\n",
    "        //continue\n",
    "    }\n",
    "    else if(colName == \"target\") {\n",
    "        println(\"We have the target columns, type \" + typeKey)\n",
    "        //continue\n",
    "    }\n",
    "    else {\n",
    "        columnsDict(typeKey.toString) ::= col.name.toString\n",
    "    }\n",
    "    \n",
    "    /*if(typeKey not in columnsDict) {\n",
    "        //columnsDict[typeKey] = [col.name]\n",
    "        columnsDict(typeKey) ::= col.name\n",
    "    }\n",
    "    else{\n",
    "        //columnsDict[typeKey].append(col.name)\n",
    "        columnsDict += (typeKey -> col.name)\n",
    "         (\"dog\") ::= \n",
    "    }*/\n",
    "}\n",
    "\n",
    "println(\"\")\n",
    "for( (ct, cl) <- columnsDict) {\n",
    "    println(ct + \" \" + cl.length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:08:07.929166",
     "start_time": "2016-03-15T22:07:34.984Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|target_freqItems|\n",
      "+----------------+\n",
      "|          [1, 0]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kaggleTrain.stat.freqItems(Seq(\"target\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:08:09.531440",
     "start_time": "2016-03-15T22:07:34.994Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+\n",
      "|target_target|    0|    1|\n",
      "+-------------+-----+-----+\n",
      "|            1|    0|87021|\n",
      "|            0|27300|    0|\n",
      "+-------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kaggleTrain.stat.crosstab(\"target\", \"target\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace String by Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:08:10.210923",
     "start_time": "2016-03-15T22:07:35.003Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Convert text field\n",
    "//var stringIndexers: ListBuffer[StringIndexer] = ListBuffer()\n",
    "var kaggleTrainDouble = kaggleTrain\n",
    "var kaggleTestDouble = kaggleTest\n",
    "var kaggleUnion = kaggleTrain.select(columnsDict(\"StringType\").head, columnsDict(\"StringType\").tail: _*).unionAll(kaggleTest.select(columnsDict(\"StringType\").head, columnsDict(\"StringType\").tail: _*))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:09:46.080297",
     "start_time": "2016-03-15T22:07:35.012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for(colName <- columnsDict(\"StringType\")) {\n",
    "    var colNameIndexed: String = colName + \"_i\"\n",
    "    \n",
    "    var labelIndexer = new StringIndexer().setInputCol(colName).setOutputCol(colNameIndexed)\n",
    "    //stringIndexers += labelIndexer\n",
    "    kaggleTrainDouble = labelIndexer.fit(kaggleUnion).transform(kaggleTrainDouble)\n",
    "    kaggleTestDouble = labelIndexer.fit(kaggleUnion).transform(kaggleTestDouble)\n",
    "    \n",
    "    columnsDict(\"StringIndexed\") ::= colNameIndexed\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:09:46.446455",
     "start_time": "2016-03-15T22:07:35.020Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Split the data into train and test\n",
    "val Array(train, test) = kaggleTrainDouble.randomSplit(Array(0.6, 0.4), seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:09:46.783076",
     "start_time": "2016-03-15T22:07:35.027Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replacementFunction(df:DataFrame) : DataFrame = {\n",
    "    val description = df.describe()\n",
    "    \n",
    "    val descriptionCol = description.collect()\n",
    "    \n",
    "    var replacementDict = new HashMap[String,Any].withDefaultValue(Nil)\n",
    "    for(col <- columnsDict(\"DoubleType\")){\n",
    "        replacementDict(col) = descriptionCol(1).getAs(col)\n",
    "    }\n",
    "    for(col <- columnsDict(\"IntegerType\")){\n",
    "        replacementDict(col) = descriptionCol(1).getAs(col)\n",
    "    }\n",
    "    //println(replacementDict)\n",
    "    println(\"Replacing!\")\n",
    "    return df.na.fill(replacementDict.toMap)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:01.443114",
     "start_time": "2016-03-15T22:07:35.034Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val description = train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:01.640951",
     "start_time": "2016-03-15T22:07:35.041Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------+\n",
      "|               v10|             target|summary|\n",
      "+------------------+-------------------+-------+\n",
      "|             68428|              68479|  count|\n",
      "|1.8805752686729722| 0.7608609938813359|   mean|\n",
      "|1.3981410306820936|0.42656089711962514| stddev|\n",
      "| -9.87531659989E-7|                  0|    min|\n",
      "|     18.5339164478|                  1|    max|\n",
      "+------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description.select(\"v10\", \"target\", \"summary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:01.888817",
     "start_time": "2016-03-15T22:07:35.047Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val descriptionCol = description.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:02.117931",
     "start_time": "2016-03-15T22:07:35.052Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean v12 example 6.879828782575871\n"
     ]
    }
   ],
   "source": [
    "println(\"Mean v12 example \" + descriptionCol(1).getAs(\"v12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:02.779420",
     "start_time": "2016-03-15T22:07:35.058Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Field \"v3\" does not exist.\n",
       "StackTrace: org.apache.spark.sql.types.StructType$$anonfun$fieldIndex$1.apply(StructType.scala:234)\n",
       "org.apache.spark.sql.types.StructType$$anonfun$fieldIndex$1.apply(StructType.scala:234)\n",
       "scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n",
       "scala.collection.AbstractMap.getOrElse(Map.scala:58)\n",
       "org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:233)\n",
       "org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:213)\n",
       "org.apache.spark.sql.Row$class.getAs(Row.scala:336)\n",
       "org.apache.spark.sql.catalyst.expressions.GenericRow.getAs(rows.scala:192)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:52)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line78.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n",
       "$line78.$read$$iwC$$iwC$$iwC.<init>(<console>:69)\n",
       "$line78.$read$$iwC$$iwC.<init>(<console>:71)\n",
       "$line78.$read$$iwC.<init>(<console>:73)\n",
       "$line78.$read.<init>(<console>:75)\n",
       "$line78.$read$.<init>(<console>:79)\n",
       "$line78.$read$.<clinit>(<console>)\n",
       "$line78.$eval$.<init>(<console>:7)\n",
       "$line78.$eval$.<clinit>(<console>)\n",
       "$line78.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:351)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:350)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:350)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsDict(\"StringType\")\n",
    "println(\"Mean v3 example\" + descriptionCol(1).getAs(\"v3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:41.515332",
     "start_time": "2016-03-15T22:07:35.064Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing!\n",
      "Replacing!\n",
      "Replacing!\n"
     ]
    }
   ],
   "source": [
    "val trainWithoutNull = replacementFunction(train)\n",
    "val testWithoutNull = replacementFunction(test)\n",
    "val kaggleTestWithoutNull = replacementFunction(kaggleTestDouble)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T18:49:41.887195",
     "start_time": "2016-03-03T17:48:31.696Z"
    },
    "collapsed": false
   },
   "source": [
    "// Repartition\n",
    "//trainWithoutNull = trainWithoutNull.repartition(20)\n",
    "//testWithoutNull = testWithoutNull.repartition(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:41.819314",
     "start_time": "2016-03-15T22:07:35.125Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainWithoutNull.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:42.649067",
     "start_time": "2016-03-15T22:07:35.131Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, v59: doub..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainWithoutNull.cache()\n",
    "testWithoutNull.cache()\n",
    "kaggleTestWithoutNull.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:42.785788",
     "start_time": "2016-03-15T22:07:35.140Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create Label\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"target\").setOutputCol(\"indexedLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:42.949487",
     "start_time": "2016-03-15T22:07:35.146Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(v3_i, v22_i, v24_i, v30_i, v31_i, v47_i, v52_i, v56_i, v66_i, v71_i, v74_i, v75_i, v79_i, v91_i, v107_i, v110_i, v112_i, v113_i, v125_i)\n"
     ]
    }
   ],
   "source": [
    "println(columnsDict(\"StringIndexed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:43.295755",
     "start_time": "2016-03-15T22:07:35.153Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create Feature vector\n",
    "val assembler = new VectorAssembler().setInputCols((columnsDict(\"IntegerType\") ++ columnsDict(\"DoubleType\") ++ columnsDict(\"StringIndexed\") ).toArray).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-02T21:11:12.588081",
     "start_time": "2016-03-02T20:11:12.572Z"
    }
   },
   "source": [
    "// output = assembler.transform(trainWithoutNull)\n",
    "// output.schema\n",
    "// trainFeat = trainWithoutNull.withColumn(\"label\", trainWithoutNull.target.cast(\"Double\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-02T21:11:33.462460",
     "start_time": "2016-03-02T20:11:33.448Z"
    },
    "collapsed": false
   },
   "source": [
    "// Automatically identify categorical features, and index them.\n",
    "// Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "//featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:43.512250",
     "start_time": "2016-03-15T22:07:35.261Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val normalizer = new Normalizer().setInputCol(\"features\").setOutputCol(\"normFeatures\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = RandomForest.trainClassifier(train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity=\"gini\", maxDepth=4, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:43.771480",
     "start_time": "2016-03-15T22:07:35.315Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Train a GBT model.\n",
    "val gbt = new RandomForestClassifier().setFeaturesCol(\"normFeatures\").setLabelCol(\"indexedLabel\").setNumTrees(150).setMaxDepth(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:10:44.164127",
     "start_time": "2016-03-15T22:07:35.322Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Chain indexer and GBT in a Pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(labelIndexer) ++ Array(assembler, normalizer, gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:19.020113",
     "start_time": "2016-03-15T22:07:35.328Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Train model.  This also runs the indexer.\n",
    "val model = pipeline.fit(trainWithoutNull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:19.208412",
     "start_time": "2016-03-15T22:07:35.336Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=rfc_d3aae16b141e) with 150 trees\n"
     ]
    }
   ],
   "source": [
    "println(model.stages(model.stages.length-1)) // summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:19.612166",
     "start_time": "2016-03-15T22:07:35.342Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val evaluator =  new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"precision\")\n",
    "val evaluator =  new BinaryClassificationEvaluator().setLabelCol(\"indexedLabel\").setRawPredictionCol(\"probability\")\n",
    "\n",
    "def evaluation(df:DataFrame) = {\n",
    "    //df.stat.crosstab(\"indexedLabel\", \"prediction\").show()\n",
    "    \n",
    "    //println(df.select(\"prediction\", \"indexedLabel\", \"probability\").take(3)) // \"rawPrediction\")\n",
    "    //println(rainPredictions.select(\"prediction\", \"indexedLabel\", \"normFeatures\").take(3))\n",
    "    //println(\"\")\n",
    "    \n",
    "    val precision = evaluator.evaluate(df)\n",
    "    println(\"Precision = \" + precision)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:21.037344",
     "start_time": "2016-03-15T22:07:35.349Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, target: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, v..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions.\n",
    "val trainPredictions = model.transform(trainWithoutNull)\n",
    "trainPredictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:33.977782",
     "start_time": "2016-03-15T22:07:35.356Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7026325538519154\n"
     ]
    }
   ],
   "source": [
    "evaluation(trainPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:34.321772",
     "start_time": "2016-03-15T22:07:35.363Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val trainPredictionsShrink = shrinkDf(trainPredictions, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:38.465697",
     "start_time": "2016-03-15T22:07:35.370Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss on Training 0.5113509034416717\n"
     ]
    }
   ],
   "source": [
    "println(\"Logloss on Training \" + logloss(trainPredictionsShrink))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:38.648591",
     "start_time": "2016-03-15T22:07:35.377Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, target: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, v..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPredictions.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:39.348025",
     "start_time": "2016-03-15T22:07:35.384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Make predictions.\n",
    "val testPredictions = model.transform(testWithoutNull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:45.668609",
     "start_time": "2016-03-15T22:07:35.390Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.691119668051505\n"
     ]
    }
   ],
   "source": [
    "evaluation(testPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:11:45.945659",
     "start_time": "2016-03-15T22:07:35.397Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testPredictionsShrink = shrinkDf(testPredictions, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:12:03.605083",
     "start_time": "2016-03-15T22:07:35.404Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss on Testing 0.5138817803019536\n"
     ]
    }
   ],
   "source": [
    "println(\"Logloss on Testing \" + logloss(testPredictionsShrink))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:12:03.868517",
     "start_time": "2016-03-15T22:07:35.414Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Grid search\n",
    "// 5->30 * 2 with 5 folds, it takes 3 days -> precision 0.851998\n",
    "//numTrees = 29\n",
    "// maxDepth <= 10\n",
    "val grid =  new ParamGridBuilder().addGrid(gbt.numTrees, Array(50, 100, 150, 200)).addGrid(gbt.maxDepth, 4 to 10).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T23:12:04.057577",
     "start_time": "2016-03-15T22:07:35.420Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Cross validation\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEstimatorParamMaps(grid).setEvaluator(evaluator).setNumFolds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:13.066985",
     "start_time": "2016-03-15T22:07:35.426Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.spark.ml.tuning.CrossValidator).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    }
   ],
   "source": [
    "val modelCv = cv.fit(trainWithoutNull)\n",
    "val predictionsCv = modelCv.transform(trainWithoutNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:21.614593",
     "start_time": "2016-03-15T22:07:35.432Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8046551457050173\n"
     ]
    }
   ],
   "source": [
    "evaluation(predictionsCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:21.738340",
     "start_time": "2016-03-15T22:07:35.438Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_fe2e8e6a9364__estimatorParamMaps\n"
     ]
    }
   ],
   "source": [
    "println(modelCv.estimatorParamMaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:22.034854",
     "start_time": "2016-03-15T22:07:35.446Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2 (default: auto)\n",
      "featuresCol: features column name (default: features, current: normFeatures)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name (default: label, current: indexedLabel)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 6)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1) (default: 20, current: 150)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "seed: random seed (default: 207336481)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rfc_ef1730d5f0ff"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sta = pipeline.getStages(3)\n",
    "print(sta.explainParams)\n",
    "sta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:29.032532",
     "start_time": "2016-03-15T22:07:35.452Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testPredictionsCv = modelCv.transform(testWithoutNull)\n",
    "evaluation(testPredictionsCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:29.280736",
     "start_time": "2016-03-15T22:07:35.458Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testPredictionsShrinkCv = shrinkDf(testPredictionsCv, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:58.262229",
     "start_time": "2016-03-15T22:07:35.464Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss on Testing CV 0.4995800614516158\n"
     ]
    }
   ],
   "source": [
    "println(\"Logloss on Testing CV \" + logloss(testPredictionsShrinkCv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:58.393784",
     "start_time": "2016-03-15T22:07:35.472Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val modelToSave =  modelCv // or model modelCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:27:58.923692",
     "start_time": "2016-03-15T22:07:35.478Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val predictions = model.transform(kaggleTestWithoutNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:28:13.254373",
     "start_time": "2016-03-15T22:07:35.485Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+---------------------+------+\n",
      "|prediction_prediction|   0.0|\n",
      "+---------------------+------+\n",
      "|                  0.0|114393|\n",
      "+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.stat.crosstab(\"prediction\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:28:13.469176",
     "start_time": "2016-03-15T22:07:35.490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val predictionsShrink = shrinkDf(predictions, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T19:10:18.960059",
     "start_time": "2016-03-16T18:10:17.863Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0,0.3407822139132458], [1,0.1692523527128953], [2,0.3204449368374288])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsShrink.select(\"ID\", \"proba\").take(3) // \"probability\", \"rawPrediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-16T00:28:19.982111",
     "start_time": "2016-03-15T22:07:35.501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val outputFile = \"results/predictionScala.csv\"\n",
    "import sys.process._\n",
    "\"rm -rf \" + outputFile !\n",
    "predictionsShrink.select(\"ID\", \"proba\").withColumnRenamed(\"proba\", \"PredictedProb\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(outputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark modifications\n",
    "custom logloss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T00:06:32.115724",
     "start_time": "2016-03-14T23:03:19.798Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:60: error: mllib is not an enclosing class\n",
       "         private[mllib] def this(scoreAndLabels: DataFrame) =\n",
       "                            ^\n",
       "<console>:43: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "       @Since(\"1.0.0\")\n",
       "        ^\n",
       "<console>:45: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "           @Since(\"1.3.0\") val scoreAndLabels: RDD[(Double, Double)],\n",
       "            ^\n",
       "<console>:46: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "           @Since(\"1.3.0\") val numBins: Int) extends Logging {\n",
       "            ^\n",
       "<console>:44: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "       class BinaryClassificationMetrics @Since(\"1.3.0\") (\n",
       "                                          ^\n",
       "<console>:53: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:61: error: overloaded constructor BinaryClassificationMetrics needs result type\n",
       "           this(scoreAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n",
       "           ^\n",
       "<console>:66: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:166: error: not found: type BinaryLabelCounter\n",
       "           cumulativeCounts: RDD[(Double, BinaryLabelCounter)],\n",
       "                                          ^\n",
       "<console>:74: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:83: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:241: error: not found: type BinaryClassificationMetricComputer\n",
       "             x: BinaryClassificationMetricComputer,\n",
       "                ^\n",
       "<console>:242: error: not found: type BinaryClassificationMetricComputer\n",
       "             y: BinaryClassificationMetricComputer): RDD[(Double, Double)] = {\n",
       "                ^\n",
       "<console>:233: error: not found: type BinaryClassificationMetricComputer\n",
       "         private def createCurve(y: BinaryClassificationMetricComputer): RDD[(Double, Double)] = {\n",
       "                                    ^\n",
       "<console>:85: error: not found: value FalsePositiveRate\n",
       "           val rocCurve = createCurve(FalsePositiveRate, Recall)\n",
       "                                      ^\n",
       "<console>:167: error: not found: type BinaryConfusionMatrix\n",
       "           confusions: RDD[(Double, BinaryConfusionMatrix)]) = {\n",
       "                                    ^\n",
       "<console>:95: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:96: error: not found: value AreaUnderCurve\n",
       "         def areaUnderROC(): Double = AreaUnderCurve.of(roc())\n",
       "                                      ^\n",
       "<console>:101: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:113: error: org.slf4j.Logger does not take parameters\n",
       "           val loglossRed2 =  loglossRed1.map{case (p,y) => y*log(p) + (1.0-y)*log(1.0-p)}\n",
       "                                                                 ^\n",
       "<console>:114: error: value + is not a member of Nothing\n",
       "           val loglossRed  =  loglossRed2.reduce((a, b) => a+b)\n",
       "                                                            ^\n",
       "<console>:124: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:126: error: not found: value Recall\n",
       "           val prCurve = createCurve(Recall, Precision)\n",
       "                                     ^\n",
       "<console>:135: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:136: error: not found: value AreaUnderCurve\n",
       "         def areaUnderPR(): Double = AreaUnderCurve.of(pr())\n",
       "                                     ^\n",
       "<console>:144: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:145: error: not found: value FMeasure\n",
       "         def fMeasureByThreshold(beta: Double): RDD[(Double, Double)] = createCurve(FMeasure(beta))\n",
       "                                                                                    ^\n",
       "<console>:150: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:156: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:157: error: not found: value Precision\n",
       "         def precisionByThreshold(): RDD[(Double, Double)] = createCurve(Precision)\n",
       "                                                                         ^\n",
       "<console>:162: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "         @Since(\"1.0.0\")\n",
       "          ^\n",
       "<console>:163: error: not found: value Recall\n",
       "         def recallByThreshold(): RDD[(Double, Double)] = createCurve(Recall)\n",
       "                                                                      ^\n",
       "<console>:171: error: not found: type BinaryLabelCounter\n",
       "             createCombiner = (label: Double) => new BinaryLabelCounter(0L, 0L) += label,\n",
       "                                                     ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//package org.apache.spark.mllib.evaluation\n",
    "\n",
    "import org.apache.spark.annotation.Since\n",
    "import org.apache.spark.Logging\n",
    "import org.apache.spark.mllib.evaluation.binary._\n",
    "import org.apache.spark.rdd.{RDD, UnionRDD}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "/**\n",
    " * Evaluator for binary classification.\n",
    " *\n",
    " * @param scoreAndLabels an RDD of (score, label) pairs.\n",
    " * @param numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally\n",
    " *                will be down-sampled to this many \"bins\". If 0, no down-sampling will occur.\n",
    " *                This is useful because the curve contains a point for each distinct score\n",
    " *                in the input, and this could be as large as the input itself -- millions of\n",
    " *                points or more, when thousands may be entirely sufficient to summarize\n",
    " *                the curve. After down-sampling, the curves will instead be made of approximately\n",
    " *                `numBins` points instead. Points are made from bins of equal numbers of\n",
    " *                consecutive points. The size of each bin is\n",
    " *                `floor(scoreAndLabels.count() / numBins)`, which means the resulting number\n",
    " *                of bins may not exactly equal numBins. The last bin in each partition may\n",
    " *                be smaller as a result, meaning there may be an extra sample at\n",
    " *                partition boundaries.\n",
    " */\n",
    "@Since(\"1.0.0\")\n",
    "class BinaryClassificationMetrics @Since(\"1.3.0\") (\n",
    "    @Since(\"1.3.0\") val scoreAndLabels: RDD[(Double, Double)],\n",
    "    @Since(\"1.3.0\") val numBins: Int) extends Logging {\n",
    "\n",
    "  require(numBins >= 0, \"numBins must be nonnegative\")\n",
    "\n",
    "  /**\n",
    "   * Defaults `numBins` to 0.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def this(scoreAndLabels: RDD[(Double, Double)]) = this(scoreAndLabels, 0)\n",
    "\n",
    "  /**\n",
    "   * An auxiliary constructor taking a DataFrame.\n",
    "   * @param scoreAndLabels a DataFrame with two double columns: score and label\n",
    "   */\n",
    "  private[mllib] def this(scoreAndLabels: DataFrame) =\n",
    "    this(scoreAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n",
    "\n",
    "  /**\n",
    "   * Unpersist intermediate RDDs used in the computation.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def unpersist() {\n",
    "    cumulativeCounts.unpersist()\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Returns thresholds in descending order.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def thresholds(): RDD[Double] = cumulativeCounts.map(_._1)\n",
    "\n",
    "  /**\n",
    "   * Returns the receiver operating characteristic (ROC) curve,\n",
    "   * which is an RDD of (false positive rate, true positive rate)\n",
    "   * with (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n",
    "   * @see http://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def roc(): RDD[(Double, Double)] = {\n",
    "    val rocCurve = createCurve(FalsePositiveRate, Recall)\n",
    "    val sc = confusions.context\n",
    "    val first = sc.makeRDD(Seq((0.0, 0.0)), 1)\n",
    "    val last = sc.makeRDD(Seq((1.0, 1.0)), 1)\n",
    "    new UnionRDD[(Double, Double)](sc, Seq(first, rocCurve, last))\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Computes the area under the receiver operating characteristic (ROC) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def areaUnderROC(): Double = AreaUnderCurve.of(roc())\n",
    "    \n",
    "  /**\n",
    "   * Computes the area under the receiver operating characteristic (ROC) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def logloss(): Double = {\n",
    "    var loglossRed1 = scoreAndLabels //df.map(r => (r.getAs[Double](\"indexedLabel\"), r.getAs[Double](\"proba\"))) //probability[1]\n",
    "    \n",
    "    // Check if(some proba are < 0\n",
    "    val neg = loglossRed1.filter{ case (p,y) => (p <= 0.0 || p >= 1.0)}\n",
    "    val negCount = neg.count()\n",
    "    if(negCount != 0) {\n",
    "        println(\"!!! There so non-valid probability !!! \" + negCount)\n",
    "        loglossRed1 = loglossRed1.filter{ case (p,y) => (p > 0.0 && p < 1.0)}\n",
    "    }\n",
    "    \n",
    "    val loglossRed2 =  loglossRed1.map{case (p,y) => y*log(p) + (1.0-y)*log(1.0-p)}\n",
    "    val loglossRed  =  loglossRed2.reduce((a, b) => a+b)\n",
    "    \n",
    "    return -1.0 * loglossRed / df.count()\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Returns the precision-recall curve, which is an RDD of (recall, precision),\n",
    "   * NOT (precision, recall), with (0.0, 1.0) prepended to it.\n",
    "   * @see http://en.wikipedia.org/wiki/Precision_and_recall\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def pr(): RDD[(Double, Double)] = {\n",
    "    val prCurve = createCurve(Recall, Precision)\n",
    "    val sc = confusions.context\n",
    "    val first = sc.makeRDD(Seq((0.0, 1.0)), 1)\n",
    "    first.union(prCurve)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Computes the area under the precision-recall curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def areaUnderPR(): Double = AreaUnderCurve.of(pr())\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, F-Measure) curve.\n",
    "   * @param beta the beta factor in F-Measure computation.\n",
    "   * @return an RDD of (threshold, F-Measure) pairs.\n",
    "   * @see http://en.wikipedia.org/wiki/F1_score\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def fMeasureByThreshold(beta: Double): RDD[(Double, Double)] = createCurve(FMeasure(beta))\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, F-Measure) curve with beta = 1.0.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def fMeasureByThreshold(): RDD[(Double, Double)] = fMeasureByThreshold(1.0)\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, precision) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def precisionByThreshold(): RDD[(Double, Double)] = createCurve(Precision)\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, recall) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def recallByThreshold(): RDD[(Double, Double)] = createCurve(Recall)\n",
    "\n",
    "  private lazy val (\n",
    "    cumulativeCounts: RDD[(Double, BinaryLabelCounter)],\n",
    "    confusions: RDD[(Double, BinaryConfusionMatrix)]) = {\n",
    "    // Create a bin for each distinct score value, count positives and negatives within each bin,\n",
    "    // and then sort by score values in descending order.\n",
    "    val counts = scoreAndLabels.combineByKey(\n",
    "      createCombiner = (label: Double) => new BinaryLabelCounter(0L, 0L) += label,\n",
    "      mergeValue = (c: BinaryLabelCounter, label: Double) => c += label,\n",
    "      mergeCombiners = (c1: BinaryLabelCounter, c2: BinaryLabelCounter) => c1 += c2\n",
    "    ).sortByKey(ascending = false)\n",
    "\n",
    "    val binnedCounts =\n",
    "      // Only down-sample if bins is > 0\n",
    "      if (numBins == 0) {\n",
    "        // Use original directly\n",
    "        counts\n",
    "      } else {\n",
    "        val countsSize = counts.count()\n",
    "        // Group the iterator into chunks of about countsSize / numBins points,\n",
    "        // so that the resulting number of bins is about numBins\n",
    "        var grouping = countsSize / numBins\n",
    "        if (grouping < 2) {\n",
    "          // numBins was more than half of the size; no real point in down-sampling to bins\n",
    "          logInfo(s\"Curve is too small ($countsSize) for $numBins bins to be useful\")\n",
    "          counts\n",
    "        } else {\n",
    "          if (grouping >= Int.MaxValue) {\n",
    "            logWarning(\n",
    "              s\"Curve too large ($countsSize) for $numBins bins; capping at ${Int.MaxValue}\")\n",
    "            grouping = Int.MaxValue\n",
    "          }\n",
    "          counts.mapPartitions(_.grouped(grouping.toInt).map { pairs =>\n",
    "            // The score of the combined point will be just the first one's score\n",
    "            val firstScore = pairs.head._1\n",
    "            // The point will contain all counts in this chunk\n",
    "            val agg = new BinaryLabelCounter()\n",
    "            pairs.foreach(pair => agg += pair._2)\n",
    "            (firstScore, agg)\n",
    "          })\n",
    "        }\n",
    "      }\n",
    "\n",
    "    val agg = binnedCounts.values.mapPartitions { iter =>\n",
    "      val agg = new BinaryLabelCounter()\n",
    "      iter.foreach(agg += _)\n",
    "      Iterator(agg)\n",
    "    }.collect()\n",
    "    val partitionwiseCumulativeCounts =\n",
    "      agg.scanLeft(new BinaryLabelCounter())(\n",
    "        (agg: BinaryLabelCounter, c: BinaryLabelCounter) => agg.clone() += c)\n",
    "    val totalCount = partitionwiseCumulativeCounts.last\n",
    "    logInfo(s\"Total counts: $totalCount\")\n",
    "    val cumulativeCounts = binnedCounts.mapPartitionsWithIndex(\n",
    "      (index: Int, iter: Iterator[(Double, BinaryLabelCounter)]) => {\n",
    "        val cumCount = partitionwiseCumulativeCounts(index)\n",
    "        iter.map { case (score, c) =>\n",
    "          cumCount += c\n",
    "          (score, cumCount.clone())\n",
    "        }\n",
    "      }, preservesPartitioning = true)\n",
    "    cumulativeCounts.persist()\n",
    "    val confusions = cumulativeCounts.map { case (score, cumCount) =>\n",
    "      (score, BinaryConfusionMatrixImpl(cumCount, totalCount).asInstanceOf[BinaryConfusionMatrix])\n",
    "    }\n",
    "    (cumulativeCounts, confusions)\n",
    "  }\n",
    "\n",
    "  /** Creates a curve of (threshold, metric). */\n",
    "  private def createCurve(y: BinaryClassificationMetricComputer): RDD[(Double, Double)] = {\n",
    "    confusions.map { case (s, c) =>\n",
    "      (s, y(c))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** Creates a curve of (metricX, metricY). */\n",
    "  private def createCurve(\n",
    "      x: BinaryClassificationMetricComputer,\n",
    "      y: BinaryClassificationMetricComputer): RDD[(Double, Double)] = {\n",
    "    confusions.map { case (_, c) =>\n",
    "      (x(c), y(c))\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-15T00:06:33.452722",
     "start_time": "2016-03-14T23:03:19.806Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:23: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "       @Since(\"1.2.0\")\n",
       "        ^\n",
       "<console>:25: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "       class BinaryClassificationEvaluator @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String)\n",
       "                                                             ^\n",
       "<console>:25: error: class Since in package annotation cannot be accessed in package org.apache.spark.annotation\n",
       "       class BinaryClassificationEvaluator @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String)\n",
       "                                            ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//package org.apache.spark.ml.evaluation\n",
    "\n",
    "import org.apache.spark.annotation.{Experimental, Since}\n",
    "import org.apache.spark.ml.param._\n",
    "import org.apache.spark.ml.param.shared._\n",
    "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable, SchemaUtils}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.linalg.{Vector, VectorUDT}\n",
    "import org.apache.spark.sql.{DataFrame, Row}\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "/**\n",
    " * :: Experimental ::\n",
    " * Evaluator for binary classification, which expects two input columns: rawPrediction and label.\n",
    " * The rawPrediction column can be of type double (binary 0/1 prediction, or probability of label 1)\n",
    " * or of type vector (length-2 vector of raw predictions, scores, or label probabilities).\n",
    " */\n",
    "@Since(\"1.2.0\")\n",
    "@Experimental\n",
    "class BinaryClassificationEvaluator @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String)\n",
    "  extends Evaluator with HasRawPredictionCol with HasLabelCol with DefaultParamsWritable {\n",
    "\n",
    "  @Since(\"1.2.0\")\n",
    "  def this() = this(Identifiable.randomUID(\"binEval\"))\n",
    "\n",
    "  /**\n",
    "   * param for metric name in evaluation (supports `\"areaUnderROC\"` (default), `\"areaUnderPR\"`)\n",
    "   * @group param\n",
    "   */\n",
    "  @Since(\"1.2.0\")\n",
    "  val metricName: Param[String] = {\n",
    "    val allowedParams = ParamValidators.inArray(Array(\"areaUnderROC\", \"areaUnderPR\", \"logloss\"))\n",
    "    new Param(\n",
    "      this, \"metricName\", \"metric name in evaluation (areaUnderROC|areaUnderPR|logloss)\", allowedParams)\n",
    "  }\n",
    "\n",
    "  /** @group getParam */\n",
    "  @Since(\"1.2.0\")\n",
    "  def getMetricName: String = $(metricName)\n",
    "\n",
    "  /** @group setParam */\n",
    "  @Since(\"1.2.0\")\n",
    "  def setMetricName(value: String): this.type = set(metricName, value)\n",
    "\n",
    "  /** @group setParam */\n",
    "  @Since(\"1.5.0\")\n",
    "  def setRawPredictionCol(value: String): this.type = set(rawPredictionCol, value)\n",
    "\n",
    "  /**\n",
    "   * @group setParam\n",
    "   * @deprecated use [[setRawPredictionCol()]] instead\n",
    "   */\n",
    "  @deprecated(\"use setRawPredictionCol instead\", \"1.5.0\")\n",
    "  @Since(\"1.2.0\")\n",
    "  def setScoreCol(value: String): this.type = set(rawPredictionCol, value)\n",
    "\n",
    "  /** @group setParam */\n",
    "  @Since(\"1.2.0\")\n",
    "  def setLabelCol(value: String): this.type = set(labelCol, value)\n",
    "\n",
    "  setDefault(metricName -> \"areaUnderROC\")\n",
    "\n",
    "  @Since(\"1.2.0\")\n",
    "  override def evaluate(dataset: DataFrame): Double = {\n",
    "    val schema = dataset.schema\n",
    "    SchemaUtils.checkColumnTypes(schema, $(rawPredictionCol), Seq(DoubleType, new VectorUDT))\n",
    "    SchemaUtils.checkColumnType(schema, $(labelCol), DoubleType)\n",
    "\n",
    "    // TODO: When dataset metadata has been implemented, check rawPredictionCol vector length = 2.\n",
    "    val scoreAndLabels = dataset.select($(rawPredictionCol), $(labelCol)).rdd.map {\n",
    "      case Row(rawPrediction: Vector, label: Double) => (rawPrediction(1), label)\n",
    "      case Row(rawPrediction: Double, label: Double) => (rawPrediction, label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    val metric = $(metricName) match {\n",
    "      case \"areaUnderROC\" => metrics.areaUnderROC()\n",
    "      case \"areaUnderPR\" => metrics.areaUnderPR()\n",
    "      case \"logloss\"  => metrics.logloss()\n",
    "    }\n",
    "    metrics.unpersist()\n",
    "    metric\n",
    "  }\n",
    "\n",
    "  @Since(\"1.5.0\")\n",
    "  override def isLargerBetter: Boolean = $(metricName) match {\n",
    "    case \"areaUnderROC\" => true\n",
    "    case \"areaUnderPR\" => true\n",
    "    case \"logloss\" => true\n",
    "  }\n",
    "\n",
    "  @Since(\"1.4.1\")\n",
    "  override def copy(extra: ParamMap): BinaryClassificationEvaluator = defaultCopy(extra)\n",
    "}\n",
    "\n",
    "@Since(\"1.6.0\")\n",
    "object BinaryClassificationEvaluator extends DefaultParamsReadable[BinaryClassificationEvaluator] {\n",
    "\n",
    "  @Since(\"1.6.0\")\n",
    "  override def load(path: String): BinaryClassificationEvaluator = super.load(path)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree",
   "language": "",
   "name": "toree"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
