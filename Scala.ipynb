{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:08.476727",
     "start_time": "2016-03-03T21:42:04.459Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{IntegerType, DoubleType, StringType, ArrayType, StructType, StructField}\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.regression.GBTRegressor\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer, VectorAssembler, Normalizer}\n",
    "import org.apache.spark.ml.evaluation.{RegressionEvaluator, MulticlassClassificationEvaluator, BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier, DecisionTreeClassifier, GBTClassifier}\n",
    "\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "\n",
    "import org.apache.spark.mllib.tree.GradientBoostedTrees\n",
    "import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n",
    "import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel\n",
    "\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "\n",
    "import collection.mutable._\n",
    "import java.io._\n",
    "import scala.math.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:08.513129",
     "start_time": "2016-03-03T21:42:04.475Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T23:33:21.630936",
     "start_time": "2016-03-03T22:33:21.354Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Log Loss metric\n",
    "def logloss(df:DataFrame) : Double = {\n",
    "    var loglossRed1 = df.map(r => (r.getAs[Double](\"indexedLabel\"), r.getAs[Double](\"proba\"))) //probability[1]\n",
    "    \n",
    "    // Check if(some proba are < 0\n",
    "    val neg = loglossRed1.filter{ case (y,p) => (p <= 0.0 || p >= 1.0)}\n",
    "    val negCount = neg.count()\n",
    "    if(negCount != 0) {\n",
    "        println(\"!!! There so non-valid probability !!! \" + negCount)\n",
    "        loglossRed1 = loglossRed1.filter{ case (y,p) => (p > 0.0 && p < 1.0)}\n",
    "    }\n",
    "    \n",
    "    val loglossRed2 =  loglossRed1.map{case (y,p) => y*log(p) + (1-y)*log(1.0-p)}\n",
    "    val loglossRed  =  loglossRed2.reduce((a, b) => a+b)\n",
    "    \n",
    "    return -1.0 * loglossRed / df.count()\n",
    "}\n",
    "//println(\"Logloss on Training \" + logloss(trainPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:11.934858",
     "start_time": "2016-03-03T21:42:04.497Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Shrink dow extrem proba\n",
    "def shrink(_value:Double, _factor:Option[Double] = Some(0.2), trunc:Double = 0.1) : Double = {\n",
    "    var value = _value\n",
    "    \n",
    "    if(value < 0.0 + trunc) {\n",
    "        value = 0.0 + trunc\n",
    "    }\n",
    "    else if(value > 1.0 - trunc) {\n",
    "        value = 1.0 - trunc\n",
    "    }\n",
    "        \n",
    "    if(_factor == None) {\n",
    "        return value\n",
    "    }\n",
    "    var factor:Double = _factor.get\n",
    "    \n",
    "    return value * (1.0-factor) + factor/2.0\n",
    "}\n",
    "//println(shrink(0.5))\n",
    "//println(shrink(0))\n",
    "//println(shrink(1))\n",
    "//println(shrink(0.95))\n",
    "//println(shrink(0.05))\n",
    "\n",
    "def shrinkDf(df:DataFrame, factor:Option[Double] = Some(0.2), trunc:Double = 0.0) : DataFrame = {\n",
    "    // proba=u\"[1,null,null,[0.9413866396761132,0.05861336032388664]]\n",
    "    //val shrinkUdf = udf(probability => shrink(float(probability.split(\",\")(4)(-2), factor, trunc), DoubleType()))\n",
    "    \n",
    "    val coder: (Vector => Double) = (probability: Vector) => {shrink(probability(1), factor, trunc)}\n",
    "    val shrinkUdf = udf(coder)\n",
    "    \n",
    "    //val dfShrink1 = df.withColumn(\"proba\", df(\"probability\"))\n",
    "    //println(dfShrink1.take(1)\n",
    "    //val dfShrink = dfShrink1.withColumn(\"proba\", shrinkUdf(dfShrink1(\"proba\")))\n",
    "    val dfShrink = df.withColumn(\"proba\", shrinkUdf(df(\"probability\")))\n",
    "    \n",
    "    return dfShrink\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:12.001309",
     "start_time": "2016-03-03T21:42:04.506Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:46: error: not found: value trainPredictions\n",
       "              trainPredictions.select(\"probability\").take(5).foreach(println)\n",
       "              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPredictions.select(\"probability\").take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:12.108585",
     "start_time": "2016-03-03T21:42:04.515Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:49: error: not found: value trainPredictions\n",
       "         val trainPredictionsShrink = shrinkDf(trainPredictions, Some(0.3))\n",
       "                                               ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainPredictionsShrink = shrinkDf(trainPredictions, Some(0.3))\n",
    "trainPredictionsShrink.select(\"probability\", \"proba\").take(5) //.select(\"proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:19.008842",
     "start_time": "2016-03-03T21:42:04.524Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, target: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, v59: d..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kaggleTrain = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"kaggle/train.csv\")\n",
    "\n",
    "kaggleTrain.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:23.053056",
     "start_time": "2016-03-03T21:42:04.531Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val kaggleTest = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").load(\"kaggle/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:35.315237",
     "start_time": "2016-03-03T21:42:04.539Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Train count 114321\n",
      "Kaggle Test count  114393\n"
     ]
    }
   ],
   "source": [
    "println(\"Kaggle Train count \" + kaggleTrain.count())\n",
    "println(\"Kaggle Test count  \" + kaggleTest.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:36.846539",
     "start_time": "2016-03-03T21:42:04.547Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the ID columns, type IntegerType\n",
      "We have the target columns, type IntegerType\n",
      "DoubleType 108\n",
      "StringType 19\n",
      "IntegerType 4\n"
     ]
    }
   ],
   "source": [
    "//println(train.schema.fields\n",
    "var columnsDict = new HashMap[String,List[String]].withDefaultValue(Nil)\n",
    "for( col <- kaggleTrain.schema.fields ) {\n",
    "    val typeKey = col.dataType\n",
    "    val colName = col.name\n",
    "    \n",
    "    if(colName == \"ID\") {\n",
    "        println(\"We have the ID columns, type \" + typeKey)\n",
    "        //continue\n",
    "    }\n",
    "    else if(colName == \"target\") {\n",
    "        println(\"We have the target columns, type \" + typeKey)\n",
    "        //continue\n",
    "    }\n",
    "    else {\n",
    "        columnsDict(typeKey.toString) ::= col.name.toString\n",
    "    }\n",
    "    \n",
    "    /*if(typeKey not in columnsDict) {\n",
    "        //columnsDict[typeKey] = [col.name]\n",
    "        columnsDict(typeKey) ::= col.name\n",
    "    }\n",
    "    else{\n",
    "        //columnsDict[typeKey].append(col.name)\n",
    "        columnsDict += (typeKey -> col.name)\n",
    "         (\"dog\") ::= \n",
    "    }*/\n",
    "}\n",
    "\n",
    "println(\"\")\n",
    "for( (ct, cl) <- columnsDict) {\n",
    "    println(ct + \" \" + cl.length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:37.543376",
     "start_time": "2016-03-03T21:42:04.556Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|target_freqItems|\n",
      "+----------------+\n",
      "|          [1, 0]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kaggleTrain.stat.freqItems(Seq(\"target\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:39.249033",
     "start_time": "2016-03-03T21:42:04.564Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----+\n",
      "|target_target|    0|    1|\n",
      "+-------------+-----+-----+\n",
      "|            1|    0|87021|\n",
      "|            0|27300|    0|\n",
      "+-------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kaggleTrain.stat.crosstab(\"target\", \"target\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:39.661926",
     "start_time": "2016-03-03T21:42:04.570Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Split the data into train and test\n",
    "val Array(train, test) = kaggleTrain.randomSplit(Array(0.6, 0.4), seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:40.029754",
     "start_time": "2016-03-03T21:42:04.576Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replacementFunction(df:DataFrame) : DataFrame = {\n",
    "    val description = df.describe()\n",
    "    \n",
    "    val descriptionCol = description.collect()\n",
    "    \n",
    "    var replacementDict = new HashMap[String,Any].withDefaultValue(Nil)\n",
    "    for(col <- columnsDict(\"DoubleType\")){\n",
    "        replacementDict(col) = descriptionCol(1).getAs(col)\n",
    "    }\n",
    "    for(col <- columnsDict(\"IntegerType\")){\n",
    "        replacementDict(col) = descriptionCol(1).getAs(col)\n",
    "    }\n",
    "    //println(replacementDict)\n",
    "    println(\"Replacing!\")\n",
    "    return df.na.fill(replacementDict.toMap)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:50.939200",
     "start_time": "2016-03-03T21:42:04.583Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val description = train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:51.206949",
     "start_time": "2016-03-03T21:42:04.590Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------+\n",
      "|               v10|             target|summary|\n",
      "+------------------+-------------------+-------+\n",
      "|             68428|              68479|  count|\n",
      "|1.8805752686729722| 0.7608609938813359|   mean|\n",
      "|1.3981410306820936|0.42656089711962514| stddev|\n",
      "| -9.87531659989E-7|                  0|    min|\n",
      "|     18.5339164478|                  1|    max|\n",
      "+------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description.select(\"v10\", \"target\", \"summary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:51.393108",
     "start_time": "2016-03-03T21:42:04.596Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val descriptionCol = description.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:51.581869",
     "start_time": "2016-03-03T21:42:04.603Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean v12 example 6.879828782575871\n"
     ]
    }
   ],
   "source": [
    "println(\"Mean v12 example \" + descriptionCol(1).getAs(\"v12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:42:52.335090",
     "start_time": "2016-03-03T21:42:04.610Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: Field \"v3\" does not exist.\n",
       "StackTrace: org.apache.spark.sql.types.StructType$$anonfun$fieldIndex$1.apply(StructType.scala:234)\n",
       "org.apache.spark.sql.types.StructType$$anonfun$fieldIndex$1.apply(StructType.scala:234)\n",
       "scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n",
       "scala.collection.AbstractMap.getOrElse(Map.scala:58)\n",
       "org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:233)\n",
       "org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:213)\n",
       "org.apache.spark.sql.Row$class.getAs(Row.scala:336)\n",
       "org.apache.spark.sql.catalyst.expressions.GenericRow.getAs(rows.scala:192)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:52)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:65)\n",
       "$line74.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:67)\n",
       "$line74.$read$$iwC$$iwC$$iwC.<init>(<console>:69)\n",
       "$line74.$read$$iwC$$iwC.<init>(<console>:71)\n",
       "$line74.$read$$iwC.<init>(<console>:73)\n",
       "$line74.$read.<init>(<console>:75)\n",
       "$line74.$read$.<init>(<console>:79)\n",
       "$line74.$read$.<clinit>(<console>)\n",
       "$line74.$eval$.<init>(<console>:7)\n",
       "$line74.$eval$.<clinit>(<console>)\n",
       "$line74.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:351)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:350)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:350)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsDict(\"StringType\")\n",
    "println(\"Mean v3 example\" + descriptionCol(1).getAs(\"v3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:43:22.232438",
     "start_time": "2016-03-03T21:42:04.618Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing!\n",
      "Replacing!\n",
      "Replacing!\n"
     ]
    }
   ],
   "source": [
    "val trainWithoutNull = replacementFunction(train)\n",
    "val testWithoutNull = replacementFunction(test)\n",
    "val kaggleTestWithoutNull = replacementFunction(kaggleTest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T18:49:41.887195",
     "start_time": "2016-03-03T17:48:31.696Z"
    },
    "collapsed": false
   },
   "source": [
    "// Repartition\n",
    "//trainWithoutNull = trainWithoutNull.repartition(20)\n",
    "//testWithoutNull = testWithoutNull.repartition(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:43:22.503128",
     "start_time": "2016-03-03T21:42:04.671Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainWithoutNull.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:43:23.214978",
     "start_time": "2016-03-03T21:42:04.679Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, v59: doub..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainWithoutNull.cache()\n",
    "testWithoutNull.cache()\n",
    "kaggleTestWithoutNull.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:43:23.477596",
     "start_time": "2016-03-03T21:42:04.686Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create Label\n",
    "val labelIndexer = new StringIndexer().setInputCol(\"target\").setOutputCol(\"indexedLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:43:23.733195",
     "start_time": "2016-03-03T21:42:04.693Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create Feature vector\n",
    "val assembler = new VectorAssembler().setInputCols((columnsDict(\"IntegerType\") ++ columnsDict(\"DoubleType\")).toArray).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-02T21:11:12.588081",
     "start_time": "2016-03-02T20:11:12.572Z"
    }
   },
   "source": [
    "// output = assembler.transform(trainWithoutNull)\n",
    "// output.schema\n",
    "// trainFeat = trainWithoutNull.withColumn(\"label\", trainWithoutNull.target.cast(\"Double\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-02T21:11:33.462460",
     "start_time": "2016-03-02T20:11:33.448Z"
    },
    "collapsed": false
   },
   "source": [
    "// Automatically identify categorical features, and index them.\n",
    "// Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "//featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:43:23.913125",
     "start_time": "2016-03-03T21:42:04.789Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val normalizer = new Normalizer().setInputCol(\"features\").setOutputCol(\"normFeatures\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = RandomForest.trainClassifier(train, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity=\"gini\", maxDepth=4, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:10:44.120810",
     "start_time": "2016-03-03T23:10:44.017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Train a GBT model.\n",
    "val gbt = new RandomForestClassifier().setFeaturesCol(\"normFeatures\").setLabelCol(\"indexedLabel\").setNumTrees(150).setMaxDepth(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:10:44.284085",
     "start_time": "2016-03-03T23:10:44.169Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Chain indexer and GBT in a Pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, labelIndexer, normalizer, gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:13.935110",
     "start_time": "2016-03-03T23:10:44.356Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Train model.  This also runs the indexer.\n",
    "val model = pipeline.fit(trainWithoutNull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:14.046405",
     "start_time": "2016-03-03T23:10:50.530Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassificationModel (uid=rfc_8e93302823a4) with 150 trees\n"
     ]
    }
   ],
   "source": [
    "println(model.stages(model.stages.length-1)) // summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:14.251054",
     "start_time": "2016-03-03T23:10:50.709Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val evaluator =  new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"precision\")\n",
    "val evaluator =  new BinaryClassificationEvaluator().setLabelCol(\"indexedLabel\").setRawPredictionCol(\"probability\")\n",
    "\n",
    "def evaluation(df:DataFrame) = {\n",
    "    //df.stat.crosstab(\"indexedLabel\", \"prediction\").show()\n",
    "    \n",
    "    //println(df.select(\"prediction\", \"indexedLabel\", \"probability\").take(3)) // \"rawPrediction\")\n",
    "    //println(rainPredictions.select(\"prediction\", \"indexedLabel\", \"normFeatures\").take(3))\n",
    "    //println(\"\")\n",
    "    \n",
    "    val precision = evaluator.evaluate(df)\n",
    "    println(\"Precision = \" + precision)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:14.612721",
     "start_time": "2016-03-03T23:10:54.787Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, target: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, ..."
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions.\n",
    "val trainPredictions = model.transform(trainWithoutNull)\n",
    "trainPredictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:22.172031",
     "start_time": "2016-03-03T23:10:55.309Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7159536152700261\n"
     ]
    }
   ],
   "source": [
    "evaluation(trainPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:20:41.213573",
     "start_time": "2016-03-03T23:20:41.049Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val trainPredictionsShrink = shrinkDf(trainPredictions, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:01.777406",
     "start_time": "2016-03-03T23:20:41.508Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss on Training 0.5045791727000289\n"
     ]
    }
   ],
   "source": [
    "println(\"Logloss on Training \" + logloss(trainPredictionsShrink))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:25.516761",
     "start_time": "2016-03-03T23:10:56.846Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ID: int, target: int, v1: double, v2: double, v3: string, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: string, v23: double, v24: string, v25: double, v26: double, v27: double, v28: double, v29: double, v30: string, v31: string, v32: double, v33: double, v34: double, v35: double, v36: double, v37: double, v38: int, v39: double, v40: double, v41: double, v42: double, v43: double, v44: double, v45: double, v46: double, v47: string, v48: double, v49: double, v50: double, v51: double, v52: string, v53: double, v54: double, v55: double, v56: string, v57: double, v58: double, ..."
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPredictions.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:25.695355",
     "start_time": "2016-03-03T23:11:00.609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Make predictions.\n",
    "val testPredictions = model.transform(testWithoutNull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:11:28.919580",
     "start_time": "2016-03-03T23:11:00.845Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7037001165087784\n"
     ]
    }
   ],
   "source": [
    "evaluation(testPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:19:40.320073",
     "start_time": "2016-03-03T23:19:40.194Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testPredictionsShrink = shrinkDf(testPredictions, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:19:49.929694",
     "start_time": "2016-03-03T23:19:40.347Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss on Testing 0.5087857488568157\n"
     ]
    }
   ],
   "source": [
    "println(\"Logloss on Testing \" + logloss(testPredictionsShrink))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:44:31.270785",
     "start_time": "2016-03-03T21:42:04.934Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Grid search\n",
    "// 5->30 * 2 with 5 folds, it takes 3 days -> precision 0.851998\n",
    "//numTrees = 29\n",
    "// maxDepth <= 10\n",
    "val grid =  new ParamGridBuilder().addGrid(gbt.numTrees, 29 to 30).addGrid(gbt.maxDepth, 10 to 11).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:44:31.438282",
     "start_time": "2016-03-03T21:42:04.940Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Cross validation\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEstimatorParamMaps(grid).setEvaluator(evaluator).setNumFolds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:51:48.803160",
     "start_time": "2016-03-03T21:42:04.946Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val modelCv = cv.fit(trainWithoutNull)\n",
    "val predictionsCv = modelCv.transform(trainWithoutNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:52:01.112836",
     "start_time": "2016-03-03T21:42:04.952Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+-----+\n",
      "|indexedLabel_prediction| 1.0|  0.0|\n",
      "+-----------------------+----+-----+\n",
      "|                    1.0|1556|14820|\n",
      "|                    0.0| 155|51948|\n",
      "+-----------------------+----+-----+\n",
      "\n",
      "[Lorg.apache.spark.sql.Row;@3eb4e2fb\n",
      "\n",
      "Precision = 0.8307759015598617\n"
     ]
    }
   ],
   "source": [
    "evaluation(predictionsCv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:52:12.744556",
     "start_time": "2016-03-03T21:42:04.959Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---+-----+\n",
      "|indexedLabel_prediction|1.0|  0.0|\n",
      "+-----------------------+---+-----+\n",
      "|                    1.0|243|10681|\n",
      "|                    0.0|149|34769|\n",
      "+-----------------------+---+-----+\n",
      "\n",
      "[Lorg.apache.spark.sql.Row;@6bc01005\n",
      "\n",
      "Precision = 0.7080827807615147\n"
     ]
    }
   ],
   "source": [
    "val testPredictionsCv = modelCv.transform(testWithoutNull)\n",
    "evaluation(testPredictionsCv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:52:13.010906",
     "start_time": "2016-03-03T21:42:04.965Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testPredictionsShrinkCv = shrinkDf(testPredictionsCv, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:52:23.471544",
     "start_time": "2016-03-03T21:42:04.971Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss on Testing CV 0.5289056659818699\n"
     ]
    }
   ],
   "source": [
    "println(\"Logloss on Testing CV \" + logloss(testPredictionsShrinkCv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:23.847917",
     "start_time": "2016-03-03T23:21:23.761Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val modelToSave =  model // or model modelCv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:24.420566",
     "start_time": "2016-03-03T23:21:24.207Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val predictions = model.transform(kaggleTestWithoutNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:31.929224",
     "start_time": "2016-03-03T23:21:24.640Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+---------------------+------+\n",
      "|prediction_prediction|   0.0|\n",
      "+---------------------+------+\n",
      "|                  0.0|114393|\n",
      "+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.stat.crosstab(\"prediction\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:32.053711",
     "start_time": "2016-03-03T23:21:25.165Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val predictionsShrink = shrinkDf(predictions, None, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:32.056860",
     "start_time": "2016-03-03T23:21:25.817Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:1: error: '=' expected but '.' found.\n",
       "       val predictionsShrink.select(\"ID\", \"proba\").take(3) // \"probability\", \"rawPrediction\"\n",
       "                                                  ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsShrink.select(\"ID\", \"proba\").take(3) // \"probability\", \"rawPrediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-04T00:21:37.158995",
     "start_time": "2016-03-03T23:21:26.411Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val outputFile = \"results/predictionScala.csv\"\n",
    "import sys.process._\n",
    "\"rm -rf \" + outputFile !\n",
    "predictionsShrink.select(\"ID\", \"proba\").withColumnRenamed(\"proba\", \"PredictedProb\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(outputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark modifications\n",
    "custom logloss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:52:39.123909",
     "start_time": "2016-03-03T21:42:05.017Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:1: error: illegal start of definition\n",
       "       package org.apache.spark.mllib.evaluation\n",
       "       ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//package org.apache.spark.mllib.evaluation\n",
    "\n",
    "import org.apache.spark.annotation.Since\n",
    "import org.apache.spark.Logging\n",
    "import org.apache.spark.mllib.evaluation.binary._\n",
    "import org.apache.spark.rdd.{RDD, UnionRDD}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "/**\n",
    " * Evaluator for binary classification.\n",
    " *\n",
    " * @param scoreAndLabels an RDD of (score, label) pairs.\n",
    " * @param numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally\n",
    " *                will be down-sampled to this many \"bins\". If 0, no down-sampling will occur.\n",
    " *                This is useful because the curve contains a point for each distinct score\n",
    " *                in the input, and this could be as large as the input itself -- millions of\n",
    " *                points or more, when thousands may be entirely sufficient to summarize\n",
    " *                the curve. After down-sampling, the curves will instead be made of approximately\n",
    " *                `numBins` points instead. Points are made from bins of equal numbers of\n",
    " *                consecutive points. The size of each bin is\n",
    " *                `floor(scoreAndLabels.count() / numBins)`, which means the resulting number\n",
    " *                of bins may not exactly equal numBins. The last bin in each partition may\n",
    " *                be smaller as a result, meaning there may be an extra sample at\n",
    " *                partition boundaries.\n",
    " */\n",
    "@Since(\"1.0.0\")\n",
    "class BinaryClassificationMetrics @Since(\"1.3.0\") (\n",
    "    @Since(\"1.3.0\") val scoreAndLabels: RDD[(Double, Double)],\n",
    "    @Since(\"1.3.0\") val numBins: Int) extends Logging {\n",
    "\n",
    "  require(numBins >= 0, \"numBins must be nonnegative\")\n",
    "\n",
    "  /**\n",
    "   * Defaults `numBins` to 0.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def this(scoreAndLabels: RDD[(Double, Double)]) = this(scoreAndLabels, 0)\n",
    "\n",
    "  /**\n",
    "   * An auxiliary constructor taking a DataFrame.\n",
    "   * @param scoreAndLabels a DataFrame with two double columns: score and label\n",
    "   */\n",
    "  private[mllib] def this(scoreAndLabels: DataFrame) =\n",
    "    this(scoreAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n",
    "\n",
    "  /**\n",
    "   * Unpersist intermediate RDDs used in the computation.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def unpersist() {\n",
    "    cumulativeCounts.unpersist()\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Returns thresholds in descending order.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def thresholds(): RDD[Double] = cumulativeCounts.map(_._1)\n",
    "\n",
    "  /**\n",
    "   * Returns the receiver operating characteristic (ROC) curve,\n",
    "   * which is an RDD of (false positive rate, true positive rate)\n",
    "   * with (0.0, 0.0) prepended and (1.0, 1.0) appended to it.\n",
    "   * @see http://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def roc(): RDD[(Double, Double)] = {\n",
    "    val rocCurve = createCurve(FalsePositiveRate, Recall)\n",
    "    val sc = confusions.context\n",
    "    val first = sc.makeRDD(Seq((0.0, 0.0)), 1)\n",
    "    val last = sc.makeRDD(Seq((1.0, 1.0)), 1)\n",
    "    new UnionRDD[(Double, Double)](sc, Seq(first, rocCurve, last))\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Computes the area under the receiver operating characteristic (ROC) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def areaUnderROC(): Double = AreaUnderCurve.of(roc())\n",
    "    \n",
    "  /**\n",
    "   * Computes the area under the receiver operating characteristic (ROC) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def logloss(): Double = {\n",
    "    var loglossRed1 = scoreAndLabels //df.map(r => (r.getAs[Double](\"indexedLabel\"), r.getAs[Double](\"proba\"))) //probability[1]\n",
    "    \n",
    "    // Check if(some proba are < 0\n",
    "    val neg = loglossRed1.filter{ case (p,y) => (p <= 0.0 || p >= 1.0)}\n",
    "    val negCount = neg.count()\n",
    "    if(negCount != 0) {\n",
    "        println(\"!!! There so non-valid probability !!! \" + negCount)\n",
    "        loglossRed1 = loglossRed1.filter{ case (p,y) => (p > 0.0 && p < 1.0)}\n",
    "    }\n",
    "    \n",
    "    val loglossRed2 =  loglossRed1.map{case (p,y) => y*log(p) + (1.0-y)*log(1.0-p)}\n",
    "    val loglossRed  =  loglossRed2.reduce((a, b) => a+b)\n",
    "    \n",
    "    return -1.0 * loglossRed / df.count()\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Returns the precision-recall curve, which is an RDD of (recall, precision),\n",
    "   * NOT (precision, recall), with (0.0, 1.0) prepended to it.\n",
    "   * @see http://en.wikipedia.org/wiki/Precision_and_recall\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def pr(): RDD[(Double, Double)] = {\n",
    "    val prCurve = createCurve(Recall, Precision)\n",
    "    val sc = confusions.context\n",
    "    val first = sc.makeRDD(Seq((0.0, 1.0)), 1)\n",
    "    first.union(prCurve)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Computes the area under the precision-recall curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def areaUnderPR(): Double = AreaUnderCurve.of(pr())\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, F-Measure) curve.\n",
    "   * @param beta the beta factor in F-Measure computation.\n",
    "   * @return an RDD of (threshold, F-Measure) pairs.\n",
    "   * @see http://en.wikipedia.org/wiki/F1_score\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def fMeasureByThreshold(beta: Double): RDD[(Double, Double)] = createCurve(FMeasure(beta))\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, F-Measure) curve with beta = 1.0.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def fMeasureByThreshold(): RDD[(Double, Double)] = fMeasureByThreshold(1.0)\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, precision) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def precisionByThreshold(): RDD[(Double, Double)] = createCurve(Precision)\n",
    "\n",
    "  /**\n",
    "   * Returns the (threshold, recall) curve.\n",
    "   */\n",
    "  @Since(\"1.0.0\")\n",
    "  def recallByThreshold(): RDD[(Double, Double)] = createCurve(Recall)\n",
    "\n",
    "  private lazy val (\n",
    "    cumulativeCounts: RDD[(Double, BinaryLabelCounter)],\n",
    "    confusions: RDD[(Double, BinaryConfusionMatrix)]) = {\n",
    "    // Create a bin for each distinct score value, count positives and negatives within each bin,\n",
    "    // and then sort by score values in descending order.\n",
    "    val counts = scoreAndLabels.combineByKey(\n",
    "      createCombiner = (label: Double) => new BinaryLabelCounter(0L, 0L) += label,\n",
    "      mergeValue = (c: BinaryLabelCounter, label: Double) => c += label,\n",
    "      mergeCombiners = (c1: BinaryLabelCounter, c2: BinaryLabelCounter) => c1 += c2\n",
    "    ).sortByKey(ascending = false)\n",
    "\n",
    "    val binnedCounts =\n",
    "      // Only down-sample if bins is > 0\n",
    "      if (numBins == 0) {\n",
    "        // Use original directly\n",
    "        counts\n",
    "      } else {\n",
    "        val countsSize = counts.count()\n",
    "        // Group the iterator into chunks of about countsSize / numBins points,\n",
    "        // so that the resulting number of bins is about numBins\n",
    "        var grouping = countsSize / numBins\n",
    "        if (grouping < 2) {\n",
    "          // numBins was more than half of the size; no real point in down-sampling to bins\n",
    "          logInfo(s\"Curve is too small ($countsSize) for $numBins bins to be useful\")\n",
    "          counts\n",
    "        } else {\n",
    "          if (grouping >= Int.MaxValue) {\n",
    "            logWarning(\n",
    "              s\"Curve too large ($countsSize) for $numBins bins; capping at ${Int.MaxValue}\")\n",
    "            grouping = Int.MaxValue\n",
    "          }\n",
    "          counts.mapPartitions(_.grouped(grouping.toInt).map { pairs =>\n",
    "            // The score of the combined point will be just the first one's score\n",
    "            val firstScore = pairs.head._1\n",
    "            // The point will contain all counts in this chunk\n",
    "            val agg = new BinaryLabelCounter()\n",
    "            pairs.foreach(pair => agg += pair._2)\n",
    "            (firstScore, agg)\n",
    "          })\n",
    "        }\n",
    "      }\n",
    "\n",
    "    val agg = binnedCounts.values.mapPartitions { iter =>\n",
    "      val agg = new BinaryLabelCounter()\n",
    "      iter.foreach(agg += _)\n",
    "      Iterator(agg)\n",
    "    }.collect()\n",
    "    val partitionwiseCumulativeCounts =\n",
    "      agg.scanLeft(new BinaryLabelCounter())(\n",
    "        (agg: BinaryLabelCounter, c: BinaryLabelCounter) => agg.clone() += c)\n",
    "    val totalCount = partitionwiseCumulativeCounts.last\n",
    "    logInfo(s\"Total counts: $totalCount\")\n",
    "    val cumulativeCounts = binnedCounts.mapPartitionsWithIndex(\n",
    "      (index: Int, iter: Iterator[(Double, BinaryLabelCounter)]) => {\n",
    "        val cumCount = partitionwiseCumulativeCounts(index)\n",
    "        iter.map { case (score, c) =>\n",
    "          cumCount += c\n",
    "          (score, cumCount.clone())\n",
    "        }\n",
    "      }, preservesPartitioning = true)\n",
    "    cumulativeCounts.persist()\n",
    "    val confusions = cumulativeCounts.map { case (score, cumCount) =>\n",
    "      (score, BinaryConfusionMatrixImpl(cumCount, totalCount).asInstanceOf[BinaryConfusionMatrix])\n",
    "    }\n",
    "    (cumulativeCounts, confusions)\n",
    "  }\n",
    "\n",
    "  /** Creates a curve of (threshold, metric). */\n",
    "  private def createCurve(y: BinaryClassificationMetricComputer): RDD[(Double, Double)] = {\n",
    "    confusions.map { case (s, c) =>\n",
    "      (s, y(c))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** Creates a curve of (metricX, metricY). */\n",
    "  private def createCurve(\n",
    "      x: BinaryClassificationMetricComputer,\n",
    "      y: BinaryClassificationMetricComputer): RDD[(Double, Double)] = {\n",
    "    confusions.map { case (_, c) =>\n",
    "      (x(c), y(c))\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-03-03T22:52:39.132195",
     "start_time": "2016-03-03T21:42:05.024Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:1: error: illegal start of definition\n",
       "       package org.apache.spark.ml.evaluation\n",
       "       ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//package org.apache.spark.ml.evaluation\n",
    "\n",
    "import org.apache.spark.annotation.{Experimental, Since}\n",
    "import org.apache.spark.ml.param._\n",
    "import org.apache.spark.ml.param.shared._\n",
    "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable, SchemaUtils}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.linalg.{Vector, VectorUDT}\n",
    "import org.apache.spark.sql.{DataFrame, Row}\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "/**\n",
    " * :: Experimental ::\n",
    " * Evaluator for binary classification, which expects two input columns: rawPrediction and label.\n",
    " * The rawPrediction column can be of type double (binary 0/1 prediction, or probability of label 1)\n",
    " * or of type vector (length-2 vector of raw predictions, scores, or label probabilities).\n",
    " */\n",
    "@Since(\"1.2.0\")\n",
    "@Experimental\n",
    "class BinaryClassificationEvaluator @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String)\n",
    "  extends Evaluator with HasRawPredictionCol with HasLabelCol with DefaultParamsWritable {\n",
    "\n",
    "  @Since(\"1.2.0\")\n",
    "  def this() = this(Identifiable.randomUID(\"binEval\"))\n",
    "\n",
    "  /**\n",
    "   * param for metric name in evaluation (supports `\"areaUnderROC\"` (default), `\"areaUnderPR\"`)\n",
    "   * @group param\n",
    "   */\n",
    "  @Since(\"1.2.0\")\n",
    "  val metricName: Param[String] = {\n",
    "    val allowedParams = ParamValidators.inArray(Array(\"areaUnderROC\", \"areaUnderPR\", \"logloss\"))\n",
    "    new Param(\n",
    "      this, \"metricName\", \"metric name in evaluation (areaUnderROC|areaUnderPR|logloss)\", allowedParams)\n",
    "  }\n",
    "\n",
    "  /** @group getParam */\n",
    "  @Since(\"1.2.0\")\n",
    "  def getMetricName: String = $(metricName)\n",
    "\n",
    "  /** @group setParam */\n",
    "  @Since(\"1.2.0\")\n",
    "  def setMetricName(value: String): this.type = set(metricName, value)\n",
    "\n",
    "  /** @group setParam */\n",
    "  @Since(\"1.5.0\")\n",
    "  def setRawPredictionCol(value: String): this.type = set(rawPredictionCol, value)\n",
    "\n",
    "  /**\n",
    "   * @group setParam\n",
    "   * @deprecated use [[setRawPredictionCol()]] instead\n",
    "   */\n",
    "  @deprecated(\"use setRawPredictionCol instead\", \"1.5.0\")\n",
    "  @Since(\"1.2.0\")\n",
    "  def setScoreCol(value: String): this.type = set(rawPredictionCol, value)\n",
    "\n",
    "  /** @group setParam */\n",
    "  @Since(\"1.2.0\")\n",
    "  def setLabelCol(value: String): this.type = set(labelCol, value)\n",
    "\n",
    "  setDefault(metricName -> \"areaUnderROC\")\n",
    "\n",
    "  @Since(\"1.2.0\")\n",
    "  override def evaluate(dataset: DataFrame): Double = {\n",
    "    val schema = dataset.schema\n",
    "    SchemaUtils.checkColumnTypes(schema, $(rawPredictionCol), Seq(DoubleType, new VectorUDT))\n",
    "    SchemaUtils.checkColumnType(schema, $(labelCol), DoubleType)\n",
    "\n",
    "    // TODO: When dataset metadata has been implemented, check rawPredictionCol vector length = 2.\n",
    "    val scoreAndLabels = dataset.select($(rawPredictionCol), $(labelCol)).rdd.map {\n",
    "      case Row(rawPrediction: Vector, label: Double) => (rawPrediction(1), label)\n",
    "      case Row(rawPrediction: Double, label: Double) => (rawPrediction, label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    val metric = $(metricName) match {\n",
    "      case \"areaUnderROC\" => metrics.areaUnderROC()\n",
    "      case \"areaUnderPR\" => metrics.areaUnderPR()\n",
    "      case \"logloss\"  => metrics.logloss()\n",
    "    }\n",
    "    metrics.unpersist()\n",
    "    metric\n",
    "  }\n",
    "\n",
    "  @Since(\"1.5.0\")\n",
    "  override def isLargerBetter: Boolean = $(metricName) match {\n",
    "    case \"areaUnderROC\" => true\n",
    "    case \"areaUnderPR\" => true\n",
    "    case \"logloss\" => true\n",
    "  }\n",
    "\n",
    "  @Since(\"1.4.1\")\n",
    "  override def copy(extra: ParamMap): BinaryClassificationEvaluator = defaultCopy(extra)\n",
    "}\n",
    "\n",
    "@Since(\"1.6.0\")\n",
    "object BinaryClassificationEvaluator extends DefaultParamsReadable[BinaryClassificationEvaluator] {\n",
    "\n",
    "  @Since(\"1.6.0\")\n",
    "  override def load(path: String): BinaryClassificationEvaluator = super.load(path)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Toree",
   "language": "",
   "name": "toree"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
